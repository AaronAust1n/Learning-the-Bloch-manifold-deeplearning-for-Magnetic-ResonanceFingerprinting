{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aqe2u5u5zhMj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "#from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "a0bILIJGXJeX",
    "outputId": "2c50eccc-daa7-4765-b8eb-1ff8b45cdb96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FRWp8xWFEijO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/content/drive/My Drive\") \n",
    "import ra\n",
    "import complexLayers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ooAm2DSWEmcB"
   },
   "source": [
    "Dataset initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fQd-Gm6sE7dM"
   },
   "source": [
    "Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MVMvtJQYzhM4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = ra.read('atoms.ra')\n",
    "train_data = torch.tensor(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nc2YiKRPj2Pw"
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "train_label = torch.tensor(ra.read('params.ra'))\n",
    "train_label1 = train_label[0,:]\n",
    "train_data =  torch.transpose(train_data, 0, 1)\n",
    "train_dataset = data.TensorDataset(train_data,train_label1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZpBoX5CwFAnT"
   },
   "source": [
    "Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hVDfFvADEjbD"
   },
   "outputs": [],
   "source": [
    "testset = ra.read('atomsroa.ra')\n",
    "test_data = torch.tensor(testset)\n",
    "test_label = torch.tensor(ra.read('paramsroa.ra'))\n",
    "test_label1 = test_label[0,:]\n",
    "test_data =  torch.transpose(test_data, 0, 1)\n",
    "test_dataset = data.TensorDataset(test_data,test_label1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0neCzmUnByRy"
   },
   "outputs": [],
   "source": [
    "trainloader = DataLoader(train_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "colab_type": "code",
    "id": "t2t-5cD-GgFa",
    "outputId": "71cd94a8-4077-45a1-cb70-4bf1ab54aeeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchbearer\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/e9/4049a47dd2e5b6346a2c5d215b0c67dce814afbab1cd54ce024533c4834e/torchbearer-0.5.3-py3-none-any.whl (138kB)\n",
      "\r",
      "\u001b[K     |██▍                             | 10kB 15.6MB/s eta 0:00:01\r",
      "\u001b[K     |████▊                           | 20kB 2.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████▏                        | 30kB 2.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▌                      | 40kB 3.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▉                    | 51kB 2.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▎                 | 61kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▋               | 71kB 3.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 81kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▍          | 92kB 3.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▊        | 102kB 3.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 112kB 3.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▌   | 122kB 3.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▉ | 133kB 3.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 143kB 3.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchbearer) (4.41.1)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from torchbearer) (1.6.0+cu101)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchbearer) (1.18.5)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->torchbearer) (0.16.0)\n",
      "Installing collected packages: torchbearer\n",
      "Successfully installed torchbearer-0.5.3\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    import torchbearer\n",
    "except:\n",
    "    !pip install torchbearer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r8i169LdHB6D"
   },
   "outputs": [],
   "source": [
    "from torchbearer import Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "sGAHKMREJzYA",
    "outputId": "dc616228-5d88-4822-8aef-1a91749c7c22"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:813: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:813: UserWarning: Using a target size (torch.Size([24])) that is different to the input size (torch.Size([24, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss 2022396.16\n",
      "Epoch 1, loss 1925135.74\n",
      "Epoch 2, loss 1828734.49\n",
      "Epoch 3, loss 1734345.99\n",
      "Epoch 4, loss 1641539.51\n",
      "Epoch 5, loss 1550666.65\n",
      "Epoch 6, loss 1463168.24\n",
      "Epoch 7, loss 1378517.28\n",
      "Epoch 8, loss 1297275.68\n",
      "Epoch 9, loss 1220384.99\n",
      "Epoch 10, loss 1147233.34\n",
      "Epoch 11, loss 1079089.38\n",
      "Epoch 12, loss 1015332.10\n",
      "Epoch 13, loss 956839.33\n",
      "Epoch 14, loss 903931.56\n",
      "Epoch 15, loss 856664.00\n",
      "Epoch 16, loss 815494.03\n",
      "Epoch 17, loss 779779.55\n",
      "Epoch 18, loss 750541.36\n",
      "Epoch 19, loss 727305.95\n",
      "Epoch 20, loss 709126.46\n",
      "Epoch 21, loss 695938.86\n",
      "Epoch 22, loss 687209.83\n",
      "Epoch 23, loss 682061.56\n",
      "Epoch 24, loss 679339.53\n",
      "Epoch 25, loss 678233.71\n",
      "Epoch 26, loss 677939.73\n",
      "Epoch 27, loss 677747.11\n",
      "Epoch 28, loss 677745.18\n",
      "Epoch 29, loss 677726.47\n",
      "Epoch 30, loss 677985.96\n",
      "Epoch 31, loss 677878.15\n",
      "Epoch 32, loss 677930.05\n",
      "Epoch 33, loss 677974.37\n",
      "Epoch 34, loss 677908.55\n",
      "Epoch 35, loss 677923.28\n",
      "Epoch 36, loss 677729.68\n",
      "Epoch 37, loss 677730.71\n",
      "Epoch 38, loss 677701.33\n",
      "Epoch 39, loss 677771.05\n",
      "Epoch 40, loss 678130.39\n",
      "Epoch 41, loss 677997.62\n",
      "Epoch 42, loss 678011.81\n",
      "Epoch 43, loss 678028.14\n",
      "Epoch 44, loss 677898.64\n",
      "Epoch 45, loss 678024.44\n",
      "Epoch 46, loss 678040.65\n",
      "Epoch 47, loss 677815.60\n",
      "Epoch 48, loss 677904.63\n",
      "Epoch 49, loss 677909.66\n",
      "Epoch 50, loss 678051.79\n",
      "Epoch 51, loss 677778.28\n",
      "Epoch 52, loss 677908.30\n",
      "Epoch 53, loss 678029.55\n",
      "Epoch 54, loss 677828.56\n",
      "Epoch 55, loss 677769.06\n",
      "Epoch 56, loss 677924.09\n",
      "Epoch 57, loss 677901.05\n",
      "Epoch 58, loss 677832.55\n",
      "Epoch 59, loss 677881.38\n",
      "Epoch 60, loss 678021.62\n",
      "Epoch 61, loss 678029.78\n",
      "Epoch 62, loss 678091.28\n",
      "Epoch 63, loss 677851.76\n",
      "Epoch 64, loss 677766.33\n",
      "Epoch 65, loss 677865.16\n",
      "Epoch 66, loss 677893.66\n",
      "Epoch 67, loss 677791.66\n",
      "Epoch 68, loss 677923.84\n",
      "Epoch 69, loss 678013.48\n",
      "Epoch 70, loss 677882.35\n",
      "Epoch 71, loss 677985.55\n",
      "Epoch 72, loss 677726.55\n",
      "Epoch 73, loss 678150.74\n",
      "Epoch 74, loss 677901.93\n",
      "Epoch 75, loss 677870.15\n",
      "Epoch 76, loss 677868.94\n",
      "Epoch 77, loss 677985.13\n",
      "Epoch 78, loss 677860.12\n",
      "Epoch 79, loss 677896.47\n",
      "Epoch 80, loss 677856.31\n",
      "Epoch 81, loss 677767.68\n",
      "Epoch 82, loss 677731.67\n",
      "Epoch 83, loss 677742.48\n",
      "Epoch 84, loss 677732.56\n",
      "Epoch 85, loss 677778.95\n",
      "Epoch 86, loss 677839.31\n",
      "Epoch 87, loss 677895.77\n",
      "Epoch 88, loss 677685.27\n",
      "Epoch 89, loss 677829.28\n",
      "Epoch 90, loss 677953.11\n",
      "Epoch 91, loss 677894.77\n",
      "Epoch 92, loss 677857.81\n",
      "Epoch 93, loss 677723.23\n",
      "Epoch 94, loss 677929.35\n",
      "Epoch 95, loss 677737.88\n",
      "Epoch 96, loss 677917.66\n",
      "Epoch 97, loss 677990.84\n",
      "Epoch 98, loss 677998.38\n",
      "Epoch 99, loss 677955.51\n",
      "**** Finished Training ****\n"
     ]
    }
   ],
   "source": [
    "seed = 7\n",
    "torch.manual_seed(seed)\n",
    "trainloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "testloader = DataLoader(test_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "\n",
    "# build the model\n",
    "model = BaselineModel()\n",
    "\n",
    "# define the loss function and the optimiser\n",
    "loss_function = nn.SmoothL1Loss()\n",
    "optimiser = optim.Adam(model.parameters())\n",
    "\n",
    "# the epoch loop\n",
    "for epoch in range(100):\n",
    "    running_loss = 0.0\n",
    "    for data in trainloader:\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # forward + loss + backward + optimise (update weights)\n",
    "        outputs = model(inputs.float())\n",
    "        loss = loss_function(outputs, torch.Tensor(labels.float()))\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        # keep track of the loss this epoch\n",
    "        running_loss += loss.item()\n",
    "    print(\"Epoch %d, loss %4.2f\" % (epoch, running_loss))\n",
    "print('**** Finished Training ****')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ezL8V44zThbo"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('BaselineModel.weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "W9-Hpv6FNqNh",
    "outputId": "9c48420e-010b-4c55-db74-1b0149bd1e45"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaselineModel(\n",
       "  (fc1): Linear(in_features=1000, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "PmUloIpqUADd",
    "outputId": "f549ddd8-b811-4520-d579-50a7b8a9aa8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([100.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([110.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([120.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([130.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([140.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([150.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([160.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([170.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([180.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([190.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([200.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([210.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([220.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([230.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([240.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([250.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([260.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([270.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([280.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([290.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([300.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([310.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([320.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([330.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([340.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([350.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([360.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([370.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([380.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([390.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([400.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([410.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([420.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([430.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([440.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([450.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([460.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([470.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([480.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([490.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([500.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([510.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([520.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([530.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([540.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([550.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([560.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([570.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([580.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([590.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([600.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([610.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([620.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([630.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([640.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([650.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([660.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([670.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([680.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([690.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([700.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([710.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([720.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([730.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([740.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([750.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([760.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([770.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([780.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([790.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([800.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([810.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([820.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([830.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([840.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([850.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([860.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([870.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([880.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([890.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([900.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([910.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([920.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([930.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([940.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([950.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([960.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([970.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([980.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([990.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([1000.,   20.,    0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([1010.,   20.,    0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([1020.,   20.,    0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([1030.,   20.,    0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([1040.,   20.,    0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([1050.,   20.,    0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([1060.,   20.,    0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([1070.,   20.,    0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([1080.,   20.,    0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([1090.,   20.,    0.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "test_data = ra.read('atomsroa.ra')\n",
    "test_label = ra.read('paramsroa.ra')\n",
    "for i in range(100):\n",
    "  test_data1 = torch.tensor(test_data[:,i]).to(device)\n",
    "  predictions = model(test_data1.float())\n",
    "  test_label1 = torch.tensor(test_label[:,i])\n",
    "  print(\"predicted parameter:\", predictions)\n",
    "  print(\"setting parameter:\", test_label1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "_ayKAdLCD774",
    "outputId": "8f9af749-956d-4efe-b6d5-59eac1a5454f"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-28e770d2f920>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrainloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtestloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TensorDataset' object has no attribute 'float'"
     ]
    }
   ],
   "source": [
    "#reset the data loaders\n",
    "seed = 7\n",
    "torch.manual_seed(seed)\n",
    "trainloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "testloader = DataLoader(test_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# build the model\n",
    "model = BaselineModel()\n",
    "\n",
    "# define the loss function and the optimiser\n",
    "loss_function = nn.SmoothL1Loss()\n",
    "optimiser = optim.Adam(model.parameters())\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "trial = Trial(model, optimiser, loss_function, metrics=['loss', 'accuracy']).to(device)\n",
    "trial.with_generators(trainloader, test_generator=testloader)\n",
    "trial.run(epochs=10)\n",
    "results = trial.evaluate(data_key=torchbearer.TEST_DATA)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kFAuL8YCzhNy"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EnW7lDihzhN1"
   },
   "outputs": [],
   "source": [
    "x = np.linspace(1, 1000, 1000)\n",
    "(data.imag[:,172]).shape\n",
    "x.shape\n",
    "area=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "BvsWdBW_zhN4",
    "outputId": "6950517e-3d39-4627-ff06-e08042ed4325",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.scatter(data.real[:,172], data.imag[:,172], label=\"star\", marker=\".\", color=\"green\")\n",
    "plt.scatter(x, data.imag[:,172], marker=\".\", color=\"blue\",s=area)\n",
    "plt.scatter(x, data.imag[:,171], marker=\".\", color=\"red\",s=area)\n",
    "plt.scatter(x, data.imag[:,0], marker=\".\", color=\"green\",s=area)\n",
    "plt.scatter(x, data.imag[:,1], marker=\".\", color=\"yellow\",s=area)\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('imaginary axis')\n",
    "plt.title('complex numbers')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wjq6SGO3zhN6"
   },
   "outputs": [],
   "source": [
    "data1 = ra.read('params.ra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "EcHf7YblzhOC",
    "outputId": "241d996d-bc7b-4b16-b4f7-f315f6d4affe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 100.,  110.,  120., ..., 3980., 3990., 4000.],\n",
       "       [  20.,   20.,   20., ..., 2000., 2000., 2000.],\n",
       "       [   0.,    0.,    0., ...,    0.,    0.,    0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bW0UhuSDzhOF",
    "outputId": "0dbb8d6b-c356-4929-847c-aad5c145d3ea",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  20.,   20.,   20., ..., 2000., 2000., 2000.])"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jeF-JxQKzhOI"
   },
   "outputs": [],
   "source": [
    "label1=data1[0,:];\n",
    "label2=data1[1,:];\n",
    "label3=data1[2,:];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mFn4veIj6rh-",
    "outputId": "6df5e72d-e0b1-4e17-f292-343916e9a4f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 100.  110.  120. ... 3980. 3990. 4000.]\n"
     ]
    }
   ],
   "source": [
    "print(label1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U4S_rQvQzhOL"
   },
   "source": [
    "When developing, it is always a good idea to initialize the random number generator to a constant to ensure that the results of your script are reproducible each time you run it. Once the model is implemented and tested we might remove this to enable variance to be captured over multiple training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pFY4irFUzhOM"
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "import numpy as np\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U-WBw45hDg3b"
   },
   "outputs": [],
   "source": [
    "from complexLayers import ComplexBatchNorm2d, ComplexConv2d, ComplexLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1pNp8uS1Fq-m"
   },
   "outputs": [],
   "source": [
    "class BaselineModel1(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BaselineModel1,self).__init__()\n",
    "        self.fc1 = ComplexLinear(1000, 512)\n",
    "        self.fc1 = ComplexLinear(512, 256)\n",
    "        self.fc3 = ComplexLinear(256, 1)\n",
    "        self.bn1 = ComplexBatchNorm2d(128)\n",
    "        self.bn2 = ComplexBatchNorm2d(128)\n",
    "             \n",
    "    def forward(self,x):\n",
    "        xr = x.real\n",
    "        xi = x.imag\n",
    "        xr,xi = self.fc1(xr,xi)\n",
    "        xr,xi = self.bn1(xr,xi)\n",
    "        xr,xi = self.fc2(xr,xi)\n",
    "        xr,xi = self.bn2(xr,xi)\n",
    "        xr,xi = self.conv2(xr,xi)\n",
    "        xr,xi = complex_relu(xr,xi)\n",
    "        xr,xi = self.fc1(xr,xi)\n",
    "        xr,xi = complex_relu(xr,xi)\n",
    "        xr,xi = self.fc3(xr,xi)\n",
    "        # take the absolute value as output\n",
    "        x = torch.sqrt(torch.pow(xr,2)+torch.pow(xi,2))\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 371
    },
    "colab_type": "code",
    "id": "2NPfqbAEXIhb",
    "outputId": "081f221d-a4ad-4cfd-e601-fc5b6bf50522"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-7360b4615722>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# forward + loss + backward + optimise (update weights)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-a5d2354fc67b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mxr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mxr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: real is not implemented for tensors with non-complex dtypes."
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "model = BaselineModel1()\n",
    "\n",
    "# define the loss function and the optimiser\n",
    "loss_function = nn.SmoothL1Loss()\n",
    "optimiser = optim.Adam(model.parameters())\n",
    "\n",
    "# the epoch loop\n",
    "for epoch in range(100):\n",
    "    running_loss = 0.0\n",
    "    for data in trainloader:\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # forward + loss + backward + optimise (update weights)\n",
    "        outputs = model(inputs.float())\n",
    "        loss = loss_function(outputs, torch.Tensor(labels.float()))\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        # keep track of the loss this epoch\n",
    "        running_loss += loss.item()\n",
    "    print(\"Epoch %d, loss %4.2f\" % (epoch, running_loss))\n",
    "print('**** Finished Training ****')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sCbHTOpRzhOQ"
   },
   "outputs": [],
   "source": [
    "def _batch_norm(x):\n",
    "  eps_ = torch.finfo(torch.float32).eps\n",
    "  y = (x - torch.mean(x)) / (torch.std(x) + eps_)\n",
    "  return y\n",
    "\n",
    "\n",
    "# define baseline model\n",
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(1000, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = _batch_norm(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        out = self.fc2(out)\n",
    "        out = _batch_norm(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "YFG1RCxmzhOS",
    "outputId": "4ee3ac6b-b3b8-4ece-8404-4f33ccedd6f4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss 117511859965.79\n",
      "Epoch 1, loss 92405531615.07\n",
      "Epoch 2, loss 93088688022.30\n",
      "Epoch 3, loss 92897486029.61\n",
      "Epoch 4, loss 92851061828.14\n",
      "Epoch 5, loss 92750755929.76\n",
      "Epoch 6, loss 92692390157.46\n",
      "Epoch 7, loss 92631273954.34\n",
      "Epoch 8, loss 92559479402.21\n",
      "Epoch 9, loss 92494858886.34\n",
      "**** Finished Training ****\n"
     ]
    }
   ],
   "source": [
    "# build the model \n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = BaselineModel(1000, 512, 256).to(device)\n",
    "\n",
    "# define the loss function and the optimiser\n",
    "loss_function = nn.MSELoss()\n",
    "optimiser = optim.Adam(model.parameters())\n",
    "\n",
    "# the epoch loop\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    i = 0\n",
    "    for i in range(data.shape[1]):\n",
    "        # get the inputs\n",
    "        \n",
    "        #inputs = torch.tensor(data.imag[:,i]) #run imag part first since real data is 0\n",
    "        #result:Expected object of scalar type Double but got scalar type Float for argument #3 'mat2' in call to _th_addmm_out\n",
    "        inputs = torch.tensor(data[:,i]).to(device)\n",
    "        labels = torch.tensor(label1[i]).to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # forward + loss + backward + optimise (update weights)\n",
    "        #print(inputs.float())\n",
    "\n",
    "        outputs = model(inputs.float())\n",
    "        loss = loss_function(outputs, torch.tensor(float(labels)))\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        # keep track of the loss this epoch\n",
    "        running_loss += loss.item()\n",
    "    print(\"Epoch %d, loss %4.2f\" % (epoch, running_loss))\n",
    "print('**** Finished Training ****')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "98SN3jbBbQPo"
   },
   "source": [
    "## Saving models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AC85zIKAbLgu"
   },
   "source": [
    "Being able to train a model is fine, but in practice once we've trained the model we probably want to save the result so we can reuse it at a later time. PyTorch makes saving the model easy using the torch.save(state, filepath) function. This will save the weights of the model so they can be loaded into a new instance at a later point.\n",
    "\n",
    "Run the following code to save the weights for use in the next part of the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uYwWauk6a7vK"
   },
   "outputs": [],
   "source": [
    "#save the trained model weights\n",
    "torch.save(model.state_dict(), \"./BaselineModel.weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z3755E9cbClj"
   },
   "source": [
    "If you are running on Colab, run the following to download the weights to the local machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "YQso-4fca_mI",
    "outputId": "30981ec5-5511-40f0-d6c4-26805d304af1"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_0bb7d23d-7d4a-4952-92ce-3b7c2b670956\", \"BaselineModel.weights\", 2578625)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_a90cd446-d1e5-4cc0-9c86-10cceb50c076\", \"BaselineModel.weights\", 2578625)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.download('BaselineModel.weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "HYmWs5-H-3y_",
    "outputId": "4c7efb78-525c-451a-d440-1646801d40e1"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_f7b00a1a-624c-472c-a622-4419ffbf5943\", \"checkpoint.pth\", 2578625)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'checkpoint.pth')\n",
    "\n",
    "# download checkpoint file\n",
    "files.download('checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AzH-0Wu9brJG",
    "outputId": "68c73ea4-c60c-4c51-9b54-a278d8492b63"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = BaselineModel(1000, 512, 256).to(device)\n",
    "model.load_state_dict(torch.load('BaselineModel.weights'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "0pG9Qqqh07DP",
    "outputId": "91d293e1-4de8-470b-bcd0-d2f1da0d9d23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([100.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([110.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([120.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([130.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([140.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([150.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([160.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([170.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([180.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([190.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([200.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([210.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([220.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([230.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([240.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([250.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([260.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([270.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([280.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([290.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([300.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([310.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([320.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([330.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([340.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([350.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([360.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([370.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([380.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([390.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([400.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([410.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([420.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([430.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([440.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([450.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([460.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([470.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([480.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([490.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([500.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([510.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([520.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([530.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([540.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([550.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([560.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([570.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([580.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([590.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([600.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([610.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([620.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([630.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([640.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([650.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([660.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([670.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([680.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([690.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([700.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([710.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([720.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([730.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([740.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([750.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([760.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([770.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([780.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([790.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([800.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([810.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([820.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([830.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([840.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([850.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([860.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([870.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([880.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([890.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([900.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([910.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([920.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([930.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([940.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([950.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([960.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([970.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([980.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([990.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([1000.,   20.,    0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([1010.,   20.,    0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([1020.,   20.,    0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([1030.,   20.,    0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([1040.,   20.,    0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([1050.,   20.,    0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([1060.,   20.,    0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([1070.,   20.,    0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([1080.,   20.,    0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2992.9651], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([1090.,   20.,    0.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "i = 0\n",
    "test_data = ra.read('atomsroa.ra')\n",
    "test_label = ra.read('paramsroa.ra')\n",
    "for i in range(100):\n",
    "  test_data1 = torch.tensor(test_data[:,i]).to(device)\n",
    "  predictions = model(test_data1.float())\n",
    "  test_label1 = torch.tensor(test_label[:,i])\n",
    "  print(\"predicted parameter:\", predictions)\n",
    "  print(\"setting parameter:\", test_label1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3raJQz8rzhOV"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Compute the model accuracy on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print('Test Accuracy: %2.2f %%' % ((100.0 * correct) / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1pLYzy7Fzo7q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pacn-7dd_56i"
   },
   "outputs": [],
   "source": [
    "class Complexcard(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Complexcard, self).__init__()\n",
    "        self.fc1 = nn.linear(1000, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "        self.bn1 = nn.BatchNorm2d\n",
    "        self.bn2 = nn.BatchNorm2d    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.bn1(x)\n",
    "        out = F.sigmoid(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.bn2(x)\n",
    "        out = F.sigmoid(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1LfkvPJSA0Pk"
   },
   "outputs": [],
   "source": [
    "class Complexsig(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm2d\n",
    "        self.activation = nn.LogSigmoid\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,1,x.shape[0])\n",
    "        x = (nn.Linear(x.shape[0],512))\n",
    "        x = self.bn(x)\n",
    "        x = activation(x)\n",
    "        x = (nn.Linear(512,256))\n",
    "        x = self.bn(x)\n",
    "        x = activation(x)\n",
    "        x = (nn.Linear(256,1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zA90bej-oIM4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QDWapIfDoPPs"
   },
   "source": [
    "If set the real part to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lof3ICInohVl",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = ra.read('atoms.ra')\n",
    "train_data = torch.tensor(dataset)\n",
    "train_data = train_data.imag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l2rB7Vfvomfg",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = ra.read('atoms.ra')\n",
    "train_data = torch.tensor(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Km6A-PPYomfj"
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "train_label = torch.tensor(ra.read('params.ra'))\n",
    "train_label1 = train_label[0,:]\n",
    "train_data =  torch.transpose(train_data, 0, 1)\n",
    "train_dataset = data.TensorDataset(train_data,train_label1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qfBlksn9omfl"
   },
   "source": [
    "Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xwQ1hYpGomfl"
   },
   "outputs": [],
   "source": [
    "testset = ra.read('atomsroa.ra')\n",
    "test_data = torch.tensor(testset)\n",
    "test_label = torch.tensor(ra.read('paramsroa.ra'))\n",
    "test_label1 = test_label[0,:]\n",
    "test_data =  torch.transpose(test_data, 0, 1)\n",
    "test_dataset = data.TensorDataset(test_data,test_label1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YVrMSYgAomfn",
    "outputId": "95b685a1-8bde-4012-8b74-77db2329b25a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:813: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:813: UserWarning: Using a target size (torch.Size([24])) that is different to the input size (torch.Size([24, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss 2022396.16\n",
      "Epoch 1, loss 1925135.74\n",
      "Epoch 2, loss 1828734.49\n",
      "Epoch 3, loss 1734345.99\n",
      "Epoch 4, loss 1641539.51\n",
      "Epoch 5, loss 1550666.65\n",
      "Epoch 6, loss 1463168.24\n",
      "Epoch 7, loss 1378517.28\n",
      "Epoch 8, loss 1297275.68\n",
      "Epoch 9, loss 1220384.99\n",
      "Epoch 10, loss 1147233.34\n",
      "Epoch 11, loss 1079089.38\n",
      "Epoch 12, loss 1015332.10\n",
      "Epoch 13, loss 956839.33\n",
      "Epoch 14, loss 903931.56\n",
      "Epoch 15, loss 856664.00\n",
      "Epoch 16, loss 815494.03\n",
      "Epoch 17, loss 779779.55\n",
      "Epoch 18, loss 750541.36\n",
      "Epoch 19, loss 727305.95\n",
      "Epoch 20, loss 709126.46\n",
      "Epoch 21, loss 695938.86\n",
      "Epoch 22, loss 687209.83\n",
      "Epoch 23, loss 682061.56\n",
      "Epoch 24, loss 679339.53\n",
      "Epoch 25, loss 678233.71\n",
      "Epoch 26, loss 677939.73\n",
      "Epoch 27, loss 677747.11\n",
      "Epoch 28, loss 677745.18\n",
      "Epoch 29, loss 677726.47\n",
      "Epoch 30, loss 677985.96\n",
      "Epoch 31, loss 677878.15\n",
      "Epoch 32, loss 677930.05\n",
      "Epoch 33, loss 677974.37\n",
      "Epoch 34, loss 677908.55\n",
      "Epoch 35, loss 677923.28\n",
      "Epoch 36, loss 677729.68\n",
      "Epoch 37, loss 677730.71\n",
      "Epoch 38, loss 677701.33\n",
      "Epoch 39, loss 677771.05\n",
      "Epoch 40, loss 678130.39\n",
      "Epoch 41, loss 677997.62\n",
      "Epoch 42, loss 678011.81\n",
      "Epoch 43, loss 678028.14\n",
      "Epoch 44, loss 677898.64\n",
      "Epoch 45, loss 678024.44\n",
      "Epoch 46, loss 678040.65\n",
      "Epoch 47, loss 677815.60\n",
      "Epoch 48, loss 677904.63\n",
      "Epoch 49, loss 677909.66\n",
      "Epoch 50, loss 678051.79\n",
      "Epoch 51, loss 677778.28\n",
      "Epoch 52, loss 677908.30\n",
      "Epoch 53, loss 678029.55\n",
      "Epoch 54, loss 677828.56\n",
      "Epoch 55, loss 677769.06\n",
      "Epoch 56, loss 677924.09\n",
      "Epoch 57, loss 677901.05\n",
      "Epoch 58, loss 677832.55\n",
      "Epoch 59, loss 677881.38\n",
      "Epoch 60, loss 678021.62\n",
      "Epoch 61, loss 678029.78\n",
      "Epoch 62, loss 678091.28\n",
      "Epoch 63, loss 677851.76\n",
      "Epoch 64, loss 677766.33\n",
      "Epoch 65, loss 677865.16\n",
      "Epoch 66, loss 677893.66\n",
      "Epoch 67, loss 677791.66\n",
      "Epoch 68, loss 677923.84\n",
      "Epoch 69, loss 678013.48\n",
      "Epoch 70, loss 677882.35\n",
      "Epoch 71, loss 677985.55\n",
      "Epoch 72, loss 677726.55\n",
      "Epoch 73, loss 678150.74\n",
      "Epoch 74, loss 677901.93\n",
      "Epoch 75, loss 677870.15\n",
      "Epoch 76, loss 677868.94\n",
      "Epoch 77, loss 677985.13\n",
      "Epoch 78, loss 677860.12\n",
      "Epoch 79, loss 677896.47\n",
      "Epoch 80, loss 677856.31\n",
      "Epoch 81, loss 677767.68\n",
      "Epoch 82, loss 677731.67\n",
      "Epoch 83, loss 677742.48\n",
      "Epoch 84, loss 677732.56\n",
      "Epoch 85, loss 677778.95\n",
      "Epoch 86, loss 677839.31\n",
      "Epoch 87, loss 677895.77\n",
      "Epoch 88, loss 677685.27\n",
      "Epoch 89, loss 677829.28\n",
      "Epoch 90, loss 677953.11\n",
      "Epoch 91, loss 677894.77\n",
      "Epoch 92, loss 677857.81\n",
      "Epoch 93, loss 677723.23\n",
      "Epoch 94, loss 677929.35\n",
      "Epoch 95, loss 677737.88\n",
      "Epoch 96, loss 677917.66\n",
      "Epoch 97, loss 677990.84\n",
      "Epoch 98, loss 677998.38\n",
      "Epoch 99, loss 677955.51\n",
      "**** Finished Training ****\n"
     ]
    }
   ],
   "source": [
    "seed = 7\n",
    "torch.manual_seed(seed)\n",
    "trainloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "#testloader = DataLoader(test_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "\n",
    "# build the model\n",
    "model = BaselineModel()\n",
    "\n",
    "# define the loss function and the optimiser\n",
    "loss_function = nn.SmoothL1Loss()\n",
    "optimiser = optim.Adam(model.parameters())\n",
    "\n",
    "# the epoch loop\n",
    "for epoch in range(100):\n",
    "    running_loss = 0.0\n",
    "    for data in trainloader:\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # forward + loss + backward + optimise (update weights)\n",
    "        outputs = model(inputs.float())\n",
    "        loss = loss_function(outputs, torch.Tensor(labels.float()))\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        # keep track of the loss this epoch\n",
    "        running_loss += loss.item()\n",
    "    print(\"Epoch %d, loss %4.2f\" % (epoch, running_loss))\n",
    "print('**** Finished Training ****')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "dQVWtX9PCElN",
    "outputId": "9262a8ec-11cf-49d3-aab0-3cfaf9ad83f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([100.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([110.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([120.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([130.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([140.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([150.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([160.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([170.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([180.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([190.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([200.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([210.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([220.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([230.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([240.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([250.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([260.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([270.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([280.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([290.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([300.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([310.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([320.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([330.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([340.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([350.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([360.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([370.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([380.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([390.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([400.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([410.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([420.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([430.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([440.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([450.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([460.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([470.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([480.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([490.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([500.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([510.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([520.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([530.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([540.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([550.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([560.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([570.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([580.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([590.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([600.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([610.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([620.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([630.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([640.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([650.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([660.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([670.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([680.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([690.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([700.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([710.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([720.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([730.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([740.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([750.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([760.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([770.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([780.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([790.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([800.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([810.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([820.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([830.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([840.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([850.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([860.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([870.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([880.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([890.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([900.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([910.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([920.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([930.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([940.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([950.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([960.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([970.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([980.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([990.,  20.,   0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([1000.,   20.,    0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([1010.,   20.,    0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([1020.,   20.,    0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([1030.,   20.,    0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([1040.,   20.,    0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([1050.,   20.,    0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([1060.,   20.,    0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([1070.,   20.,    0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([1080.,   20.,    0.], dtype=torch.float64)\n",
      "predicted parameter: tensor([2508.2239], grad_fn=<AddBackward0>)\n",
      "setting parameter: tensor([1090.,   20.,    0.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "i = 0\n",
    "test_data = ra.read('atomsroa.ra')\n",
    "test_label = ra.read('paramsroa.ra')\n",
    "for i in range(100):\n",
    "  test_data1 = torch.tensor(test_data[:,i])\n",
    "  predictions = model(test_data1.float())\n",
    "  test_label1 = torch.tensor(test_label[:,i])\n",
    "  print(\"predicted parameter:\", predictions)\n",
    "  print(\"setting parameter:\", test_label1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "q50Lg9YWFA3l",
    "outputId": "4a1df9f8-dca0-4bae-94ef-a2ed0390c13a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0022, -0.0190,  0.0101,  ..., -0.0160, -0.0058,  0.0304],\n",
      "        [ 0.0280,  0.0162, -0.0149,  ..., -0.0258, -0.0183,  0.0006],\n",
      "        [-0.0032,  0.0069,  0.0123,  ..., -0.0170, -0.0314,  0.0168],\n",
      "        ...,\n",
      "        [ 0.0210, -0.0096,  0.0019,  ..., -0.0288,  0.0015, -0.0260],\n",
      "        [ 0.0172,  0.0048, -0.0126,  ...,  0.0291, -0.0085, -0.0152],\n",
      "        [-0.0011,  0.0265,  0.0007,  ..., -0.0177,  0.0052,  0.0150]])\n",
      "tensor([ 1.3176e-01, -1.6505e-01, -1.8335e-01,  1.4977e-01,  5.3468e-02,\n",
      "         3.1411e-01,  5.6621e-01, -1.3339e-02, -4.9284e-02,  4.7443e-01,\n",
      "        -1.0569e-02,  2.0607e-02, -1.0076e-01,  1.4511e-01,  2.8492e-01,\n",
      "        -1.3758e-01,  3.6764e-01,  2.0254e-01,  4.6537e-01, -8.2923e-02,\n",
      "         4.6460e-01,  5.4099e-01, -3.0317e-01, -2.1549e-01, -2.2509e-01,\n",
      "        -1.5506e-01, -1.0876e-01,  3.2170e-01,  1.2305e-01, -3.0932e-01,\n",
      "        -1.2955e-01,  6.4036e-01,  1.5676e-01, -2.4657e-01,  1.1528e-01,\n",
      "        -1.2204e-01,  5.5772e-02, -1.2193e-01,  9.6900e-02,  4.9912e-01,\n",
      "        -2.4024e-01, -6.2817e-02,  2.7682e-01, -2.6553e-01, -3.8740e-02,\n",
      "        -1.8260e-01,  1.4400e-02, -1.6539e-01, -1.2059e-01, -4.3419e-02,\n",
      "        -7.4296e-02,  1.7840e-01,  1.4326e-01,  2.5993e-01, -8.6396e-02,\n",
      "        -1.0358e-01,  6.2221e-01, -1.4528e-01, -1.5797e-01, -7.1970e-02,\n",
      "         2.7620e-01, -6.8970e-02, -1.8870e-01,  2.1550e-01, -1.8973e-01,\n",
      "         2.9727e-01,  3.9533e-01,  1.8557e-01,  3.8334e-01, -2.1524e-01,\n",
      "        -2.2461e-01, -2.2728e-01,  3.5788e-01, -1.5490e-01,  5.5005e-01,\n",
      "        -1.3982e-01, -4.7735e-02,  8.0746e-02,  1.0219e-01, -1.2543e-01,\n",
      "        -1.6571e-02,  2.9347e-02, -2.3310e-01, -2.9705e-01, -5.1003e-05,\n",
      "        -1.8045e-01,  5.0907e-01, -1.7685e-01, -2.0494e-01, -2.2991e-01,\n",
      "         4.3448e-01, -2.6273e-01, -1.2135e-01, -1.4775e-01, -1.9895e-01,\n",
      "        -7.5369e-02, -1.5112e-01,  2.9482e-01,  3.0460e-01, -1.4375e-01,\n",
      "         1.2495e-01,  4.3968e-02,  5.2164e-01,  8.1257e-02, -6.2980e-02,\n",
      "         1.2798e-01,  8.3478e-02, -1.9023e-01, -5.6071e-02, -2.4913e-01,\n",
      "        -1.0992e-01,  1.1400e-02,  1.6226e-01,  7.0419e-02, -2.5423e-01,\n",
      "        -2.1091e-02,  2.6139e-01,  5.0297e-01, -1.6609e-01, -1.5615e-01,\n",
      "         2.7102e-01,  1.4205e-01, -2.7138e-01, -1.0396e-01,  3.6557e-01,\n",
      "         2.0191e-01, -1.2666e-01, -5.6244e-02, -1.5487e-01,  2.3762e-01,\n",
      "         1.5658e-01, -2.4022e-01, -2.2667e-01, -1.6352e-01, -2.0134e-01,\n",
      "        -4.1589e-03, -2.5278e-01,  4.3098e-01,  2.7500e-02,  2.4279e-01,\n",
      "        -2.6275e-01, -2.3803e-01,  3.3672e-01,  5.5360e-01,  1.6599e-01,\n",
      "         1.7500e-01, -1.1527e-01,  5.3502e-01, -5.7558e-02,  4.2360e-01,\n",
      "        -1.5417e-01,  3.5646e-01,  3.9408e-01, -1.8797e-01, -1.0520e-01,\n",
      "        -1.7758e-02,  6.0105e-02, -2.5295e-01, -1.6546e-01, -3.1351e-01,\n",
      "        -2.6060e-01, -1.2518e-01, -7.0934e-02, -1.4891e-01, -1.3814e-01,\n",
      "         1.6418e-02, -1.4295e-01, -5.6146e-02, -2.8896e-02, -1.6630e-01,\n",
      "         1.5946e-01,  3.8824e-01, -1.2293e-01, -2.6318e-01, -1.5990e-02,\n",
      "        -1.7396e-01,  2.6758e-02, -1.8983e-01, -2.3144e-02,  3.4971e-01,\n",
      "        -8.1330e-02, -2.1187e-01, -5.1871e-02, -2.3108e-02, -1.3983e-01,\n",
      "        -7.9221e-02, -2.2826e-01, -9.9984e-02, -1.6405e-01, -2.9818e-02,\n",
      "        -1.4998e-01, -1.6457e-01,  1.6016e-01,  4.8798e-01,  1.7616e-01,\n",
      "         2.3379e-01, -1.4977e-01,  3.0274e-01, -2.5857e-01, -2.2449e-01,\n",
      "        -1.0539e-01,  5.1140e-01,  1.6365e-01, -1.7105e-01,  4.1459e-01,\n",
      "         4.3848e-01,  1.7124e-01, -1.6406e-01, -9.3963e-02, -4.4852e-03,\n",
      "         1.8272e-01,  4.2427e-01, -1.0433e-01,  5.8163e-01, -2.6111e-01,\n",
      "        -2.7399e-01,  1.7469e-01, -1.7919e-01, -6.6660e-02,  1.0068e-01,\n",
      "        -2.4727e-01, -2.9092e-01, -1.7168e-01, -1.4983e-01, -2.9001e-01,\n",
      "         4.6653e-01, -2.5653e-01, -6.3311e-02,  2.5794e-01,  2.7579e-02,\n",
      "         8.8247e-02, -7.8573e-02, -7.2238e-02,  2.2612e-01,  1.3297e-01,\n",
      "         4.4294e-01, -6.5690e-02, -1.5424e-03,  4.7609e-01, -2.9517e-01,\n",
      "         2.0132e-02,  8.7233e-02, -2.0264e-01,  1.9006e-01,  4.2896e-01,\n",
      "        -1.0913e-01, -1.6956e-02, -7.4382e-02, -1.4924e-01, -1.9235e-01,\n",
      "        -2.1311e-01, -3.4718e-02,  5.8188e-01,  9.3772e-02, -4.3623e-02,\n",
      "        -1.5112e-01, -2.6230e-01, -1.5561e-01, -6.0503e-02, -1.7501e-01,\n",
      "        -1.1238e-01,  2.7071e-01, -3.8398e-02,  2.7196e-02,  4.5868e-01,\n",
      "        -5.8177e-02,  1.2570e-01, -1.3732e-01,  3.4334e-01, -1.5136e-02,\n",
      "        -1.6442e-01,  1.6484e-02, -1.3387e-02, -2.3286e-01, -1.3628e-01,\n",
      "        -1.6339e-01,  4.6288e-01, -3.4523e-02,  1.8793e-01,  2.9524e-01,\n",
      "        -3.5489e-02,  2.7195e-01, -2.0151e-01, -2.5814e-01, -9.4276e-02,\n",
      "        -2.5736e-02, -1.4901e-01,  1.5699e-03,  2.5076e-02, -1.0129e-01,\n",
      "        -1.2376e-01,  4.3567e-02, -2.6335e-01,  1.9155e-01,  1.2980e-01,\n",
      "        -4.8368e-02,  1.9808e-01, -4.8925e-02, -2.2200e-01, -9.4823e-02,\n",
      "        -1.4170e-01, -1.6292e-01, -1.7862e-01,  3.0216e-01,  6.5219e-03,\n",
      "         2.0736e-01,  3.1725e-01,  3.7366e-02,  3.5682e-01, -2.4513e-01,\n",
      "         4.1949e-01, -8.0822e-02, -9.3038e-02,  2.4849e-01, -9.8598e-02,\n",
      "         2.2089e-01, -9.3231e-02,  4.0750e-01,  5.2234e-02, -1.7634e-01,\n",
      "        -1.2095e-01, -1.0759e-01,  2.4708e-02, -2.0457e-01, -1.0019e-01,\n",
      "         1.1267e-02, -1.0496e-01,  3.5903e-01, -2.7326e-01, -1.1498e-01,\n",
      "         1.9682e-01, -2.8553e-01,  1.0635e-01, -2.6317e-01,  3.1870e-01,\n",
      "        -2.6563e-01, -3.0726e-01, -3.4466e-02,  1.1378e-01, -5.2122e-02,\n",
      "         5.9283e-01,  2.5488e-01, -1.0470e-01,  4.7743e-01, -4.5217e-02,\n",
      "        -2.3165e-01, -2.7211e-01,  5.3123e-01,  2.3030e-01, -1.3000e-02,\n",
      "         4.2691e-01, -2.8748e-01,  4.0942e-01, -1.7268e-01, -2.9519e-01,\n",
      "         4.0782e-01, -2.0471e-01, -2.3801e-01,  6.1586e-02, -1.2788e-01,\n",
      "         2.9916e-01, -4.4238e-02, -4.0317e-02, -2.0659e-01,  1.5345e-02,\n",
      "        -2.2840e-01, -1.5877e-01,  4.9394e-01, -1.9223e-01, -7.9608e-02,\n",
      "        -2.2893e-01, -7.5273e-02, -1.8149e-01, -1.7192e-01,  4.0902e-01,\n",
      "        -1.0309e-01, -1.6919e-01, -2.0267e-01,  8.9430e-03, -1.2702e-01,\n",
      "        -3.2387e-01, -1.3230e-01,  3.7324e-01, -4.2693e-04,  1.7158e-01,\n",
      "         1.9638e-01, -9.0517e-02,  3.0959e-01,  4.4701e-01, -2.8088e-01,\n",
      "        -3.0040e-02, -1.0535e-01, -1.4939e-01, -1.7861e-01,  3.8281e-02,\n",
      "        -1.1695e-01, -1.7122e-01, -2.2382e-01, -1.4962e-01,  1.5922e-01,\n",
      "         3.9473e-01,  4.7187e-01, -1.2651e-01, -5.3230e-02, -1.5005e-02,\n",
      "        -1.0772e-01, -1.0410e-01, -1.7026e-01,  3.2038e-01, -1.1443e-01,\n",
      "        -1.3403e-01, -2.6263e-01, -2.1960e-01, -2.1427e-01,  7.0209e-02,\n",
      "         2.6566e-01,  1.5522e-01, -2.3742e-01, -1.9757e-01, -1.4873e-01,\n",
      "        -3.5448e-01, -2.6980e-01, -1.6644e-02, -1.8075e-01, -2.1900e-01,\n",
      "         2.9654e-01,  3.0777e-01, -1.7755e-01, -6.3193e-02, -6.0142e-03,\n",
      "         4.1490e-01,  1.0288e-01, -1.2192e-01,  4.6338e-01, -2.6583e-01,\n",
      "         8.4540e-02, -2.4465e-01, -1.5117e-02, -1.5314e-01,  2.0524e-01,\n",
      "         4.7964e-02,  2.3347e-01, -7.6437e-02,  1.1518e-01, -1.7833e-01,\n",
      "        -1.1158e-01,  1.6404e-01,  2.9172e-01, -5.0702e-02, -1.7080e-01,\n",
      "         4.2773e-01, -8.5964e-02, -1.0698e-01,  9.5580e-03,  6.1672e-02,\n",
      "        -1.1253e-01,  7.6318e-02,  1.7644e-01, -1.3772e-01, -7.0020e-02,\n",
      "        -1.6390e-01, -2.2985e-01, -2.4660e-01, -9.7059e-02, -3.0466e-01,\n",
      "         4.1014e-01, -3.2133e-01, -1.0448e-01, -2.2997e-01, -1.3854e-01,\n",
      "         5.0620e-02, -1.6097e-01, -2.1167e-01, -1.8033e-01,  1.1475e-01,\n",
      "        -3.3108e-02, -1.9729e-01,  1.0970e-01,  1.0558e-01,  5.6273e-01,\n",
      "         3.9555e-02, -2.4694e-01, -1.5735e-01,  4.4008e-01, -1.4409e-01,\n",
      "        -1.6459e-01,  2.1457e-01, -2.2511e-01, -2.2165e-01,  4.4787e-01,\n",
      "         2.8645e-01,  1.9279e-01, -2.5072e-01,  3.9840e-01,  1.0461e-01,\n",
      "         7.7217e-03,  1.1351e-01, -8.4480e-02, -1.7693e-01, -4.9818e-02,\n",
      "         2.6609e-01, -1.1153e-01,  2.0941e-02, -1.0444e-01,  6.5797e-02,\n",
      "        -1.4793e-01, -2.0582e-01,  1.4475e-01, -2.4067e-01, -1.0874e-01,\n",
      "        -1.7892e-01,  4.2223e-02])\n",
      "tensor([[0.0383, 0.0977, 0.0357,  ..., 0.0767, 0.0437, 0.0658],\n",
      "        [0.0455, 0.0458, 0.0263,  ..., 0.0870, 0.0243, 0.0800],\n",
      "        [0.1010, 0.0613, 0.0968,  ..., 0.0890, 0.0286, 0.0490],\n",
      "        ...,\n",
      "        [0.1020, 0.1234, 0.0855,  ..., 0.0342, 0.0604, 0.1222],\n",
      "        [0.0315, 0.0408, 0.0954,  ..., 0.0838, 0.1074, 0.0560],\n",
      "        [0.0324, 0.0384, 0.0296,  ..., 0.0718, 0.0386, 0.0819]])\n",
      "tensor([ 0.0810,  0.0490,  0.0456,  0.0773,  0.0418,  0.0576,  0.0594,  0.0945,\n",
      "         0.0529, -1.3997,  0.0704,  0.0288, -1.3340,  0.0744,  0.1114,  0.0677,\n",
      "         0.0608,  0.0341, -1.4243,  0.0130,  0.0980,  0.0708,  0.0449,  0.0208,\n",
      "         0.0224,  0.0558,  0.0600,  0.1049,  0.0415,  0.0662,  0.0878,  0.0743,\n",
      "         0.0586,  0.0378,  0.0363,  0.0669,  0.0659,  0.0381,  0.0704,  0.0603,\n",
      "        -1.4467,  0.0322,  0.0611,  0.0214,  0.0644,  0.1048,  0.0956,  0.0341,\n",
      "         0.0412,  0.1051,  0.0258, -1.3008,  0.0384,  0.0489, -1.4209,  0.0835,\n",
      "         0.1061,  0.0859, -1.4073,  0.0407,  0.1273,  0.0497,  0.0640,  0.0869,\n",
      "        -1.4004,  0.0377, -1.4023,  0.0413,  0.0474,  0.0488,  0.0888,  0.0811,\n",
      "         0.0924,  0.0204, -1.3629,  0.0647,  0.0727,  0.0978,  0.0382, -1.4060,\n",
      "         0.0397,  0.0237,  0.0547, -1.4089,  0.0500,  0.0483,  0.0860,  0.0750,\n",
      "        -1.3884,  0.0227,  0.0218,  0.0956,  0.0881,  0.0453,  0.0323,  0.0429,\n",
      "         0.0984,  0.0303,  0.0529,  0.0462,  0.0796,  0.0919,  0.0690,  0.0728,\n",
      "         0.0670,  0.0419,  0.0930,  0.0799,  0.0902,  0.0115,  0.0955,  0.0828,\n",
      "         0.0911,  0.0386,  0.0452,  0.0975,  0.0216,  0.1003,  0.1523,  0.0937,\n",
      "         0.0734,  0.0362,  0.0988,  0.0537,  0.1079,  0.0497,  0.0862,  0.1035,\n",
      "        -1.4095,  0.1022,  0.1027,  0.0518,  0.1041,  0.0366,  0.0457,  0.0284,\n",
      "         0.0761,  0.0879,  0.0513,  0.1008, -1.3634,  0.0611,  0.0769, -1.3431,\n",
      "         0.0433,  0.1242,  0.0960,  0.0662,  0.0429,  0.0663,  0.0545,  0.0093,\n",
      "         0.1048,  0.0859,  0.0762,  0.0232,  0.1071,  0.0857,  0.0600,  0.0353,\n",
      "         0.0511, -1.3927,  0.0546,  0.0700,  0.0810,  0.0675,  0.0588,  0.0476,\n",
      "         0.0823,  0.0445,  0.0838,  0.0412,  0.0823,  0.0546, -1.3780,  0.0449,\n",
      "         0.0612,  0.0821,  0.0944,  0.0427,  0.0829, -1.4229,  0.0880,  0.0865,\n",
      "         0.0423,  0.0538,  0.0927,  0.0218,  0.0807,  0.0840,  0.0631,  0.0414,\n",
      "        -1.3629, -1.4254,  0.0253,  0.0372,  0.0521,  0.1110,  0.0793,  0.0981,\n",
      "        -1.2502,  0.0210,  0.0621,  0.0519,  0.0925,  0.0359,  0.0788,  0.0394,\n",
      "         0.0950, -1.3743,  0.1177,  0.0553,  0.0338,  0.0966,  0.0995,  0.0669,\n",
      "         0.0822,  0.0229,  0.0313,  0.0783,  0.0201,  0.0390,  0.0932,  0.0957,\n",
      "         0.1034,  0.0462,  0.0964,  0.0797,  0.1009,  0.0250,  0.0437,  0.0224,\n",
      "         0.0732,  0.0483,  0.0770,  0.0957, -1.4383,  0.0303, -1.3407, -1.3894,\n",
      "         0.0280, -1.2931,  0.0650, -1.3747,  0.0904,  0.0220,  0.0475,  0.1012,\n",
      "         0.0330, -1.3672,  0.0364,  0.1086,  0.0265,  0.0357,  0.0790,  0.0227])\n",
      "tensor([[18.4102, 18.4833, 18.4106, 18.3638, 18.3594, 18.7894, 18.4097, 18.4316,\n",
      "         18.3782, 17.5479, 18.3934, 18.3644, 17.7238, 18.4392, 18.6873, 18.4186,\n",
      "         18.3731, 18.4401, 17.5730, 18.3465, 18.3888, 18.4646, 18.3417, 18.3389,\n",
      "         18.4168, 18.5202, 18.3496, 18.3892, 18.4032, 18.3923, 18.3925, 18.4966,\n",
      "         18.3833, 18.3584, 18.4111, 18.3679, 18.4830, 18.3477, 18.4165, 18.3346,\n",
      "         17.4430, 18.3846, 18.3782, 18.4011, 18.4214, 18.4301, 18.3975, 18.3501,\n",
      "         18.3510, 18.5271, 18.3744, 17.7503, 18.3700, 18.4401, 17.5150, 18.4588,\n",
      "         18.9226, 18.4591, 17.5025, 18.3418, 18.9866, 18.4008, 18.4411, 18.3722,\n",
      "         17.5788, 18.3560, 17.6680, 18.3478, 18.3707, 18.3369, 18.3586, 18.4006,\n",
      "         18.4381, 18.4554, 17.6177, 19.0563, 18.3607, 18.3932, 18.3497, 17.6482,\n",
      "         18.3454, 18.3410, 18.4439, 17.6712, 18.4119, 18.4495, 18.3784, 18.4867,\n",
      "         17.5048, 18.3686, 18.3453, 18.4111, 18.4360, 18.8845, 18.5467, 18.3530,\n",
      "         18.4714, 18.4192, 18.3852, 18.3421, 18.4088, 18.4121, 18.4396, 18.4350,\n",
      "         18.3861, 18.4139, 18.4054, 18.3933, 18.3386, 18.3638, 18.5177, 18.4363,\n",
      "         18.3647, 18.5574, 18.4016, 18.3428, 18.3430, 18.5321, 19.1773, 18.3458,\n",
      "         18.3510, 18.3879, 18.4119, 18.3408, 18.6325, 18.3944, 18.4060, 18.3560,\n",
      "         17.6223, 18.4024, 18.4171, 18.3794, 18.4216, 18.3595, 18.3995, 18.3904,\n",
      "         18.3735, 18.4292, 18.3524, 18.4036, 17.6860, 18.3722, 18.3928, 17.6788,\n",
      "         18.4593, 18.3577, 18.4273, 18.3431, 18.3372, 18.4682, 18.3520, 18.5727,\n",
      "         19.1048, 18.3554, 18.4103, 18.3918, 18.5504, 18.4848, 18.4401, 18.3397,\n",
      "         18.3417, 17.1584, 18.3794, 18.4210, 18.4428, 18.4058, 18.3763, 18.4278,\n",
      "         18.4655, 18.4809, 18.3395, 18.5485, 18.3984, 18.3664, 17.7123, 18.3361,\n",
      "         18.3876, 18.4358, 18.6304, 18.4593, 18.4467, 17.6238, 18.3797, 18.4045,\n",
      "         18.3503, 18.3722, 18.4252, 18.3914, 18.5705, 18.4769, 18.3699, 18.3418,\n",
      "         17.4035, 17.5221, 18.3808, 18.3571, 18.4228, 18.6709, 18.4302, 18.4026,\n",
      "         17.7288, 18.3494, 18.3887, 18.3817, 18.4060, 18.3992, 18.3828, 18.3407,\n",
      "         18.3870, 17.5324, 18.3493, 18.4049, 18.6743, 18.4103, 18.3774, 18.3343,\n",
      "         18.3696, 18.3880, 18.3595, 18.5060, 18.3749, 18.4090, 18.3482, 18.4198,\n",
      "         18.4335, 18.4087, 18.4292, 18.3615, 18.3582, 18.3594, 18.3548, 18.3798,\n",
      "         18.3731, 18.4113, 18.4095, 18.3415, 17.4682, 18.4118, 17.6549, 17.7210,\n",
      "         18.3555, 17.6800, 18.6032, 17.6175, 18.3490, 18.4275, 18.3428, 18.4134,\n",
      "         18.3744, 17.6421, 18.3609, 18.6459, 18.3643, 18.3610, 18.3405, 18.3531]])\n",
      "tensor([18.4499])\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "  print(param.data)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ComplexNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
